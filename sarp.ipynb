{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "import pandas as pd\n",
    "from pandas import ExcelFile\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import tensorflow as tf\n",
    "import scipy.io as io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification of Asthma dataset using different machine learning tools\n",
    "\n",
    "We have a datset provided by SARP study, containing spirometery and demographic data for 1358 subjects. The goal of this project is to classify and predict asthma severity using Age of hospital enrolment, Age of Asthma onset, Sex and FEV/FVC ratio of a patient.\n",
    "\n",
    "## This Section is for data preperation. We load the excel files in a pandas dataframe and explore it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an Excel file object in pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xl = pd.ExcelFile(\"sarp.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the sheet name in the excael file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sarp']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xl.sheet_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the information in the excel object in a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xl.parse('sarp')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploring the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>gender</th>\n",
       "      <th>Age_Enroll</th>\n",
       "      <th>ageasthonset</th>\n",
       "      <th>asthma_status</th>\n",
       "      <th>Baseline_preDrug_FEV1pp</th>\n",
       "      <th>Baseline_preDrug_FVCpp</th>\n",
       "      <th>Baseline_preDrug_FEV1_FVC</th>\n",
       "      <th>status_factorized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>22.753425</td>\n",
       "      <td>7</td>\n",
       "      <td>mild</td>\n",
       "      <td>97</td>\n",
       "      <td>137.0</td>\n",
       "      <td>0.622000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>25.726027</td>\n",
       "      <td>6</td>\n",
       "      <td>mild</td>\n",
       "      <td>94</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.770197</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>21.893151</td>\n",
       "      <td>11</td>\n",
       "      <td>mild</td>\n",
       "      <td>114</td>\n",
       "      <td>114.0</td>\n",
       "      <td>0.885287</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>39.827397</td>\n",
       "      <td>27</td>\n",
       "      <td>mild</td>\n",
       "      <td>100</td>\n",
       "      <td>105.0</td>\n",
       "      <td>0.761993</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>20.098630</td>\n",
       "      <td>15</td>\n",
       "      <td>mild</td>\n",
       "      <td>80</td>\n",
       "      <td>93.0</td>\n",
       "      <td>0.727473</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID  gender  Age_Enroll  ageasthonset asthma_status  \\\n",
       "0   1       2   22.753425             7          mild   \n",
       "1   2       1   25.726027             6          mild   \n",
       "2   3       2   21.893151            11          mild   \n",
       "3   4       1   39.827397            27          mild   \n",
       "4   5       1   20.098630            15          mild   \n",
       "\n",
       "   Baseline_preDrug_FEV1pp  Baseline_preDrug_FVCpp  Baseline_preDrug_FEV1_FVC  \\\n",
       "0                       97                   137.0                   0.622000   \n",
       "1                       94                   100.0                   0.770197   \n",
       "2                      114                   114.0                   0.885287   \n",
       "3                      100                   105.0                   0.761993   \n",
       "4                       80                    93.0                   0.727473   \n",
       "\n",
       "   status_factorized  \n",
       "0                  1  \n",
       "1                  1  \n",
       "2                  1  \n",
       "3                  1  \n",
       "4                  1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if there is any missing values in the columns we need as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID                           1358\n",
       "gender                       1358\n",
       "Age_Enroll                   1358\n",
       "ageasthonset                 1358\n",
       "asthma_status                1358\n",
       "Baseline_preDrug_FEV1pp      1358\n",
       "Baseline_preDrug_FVCpp       1324\n",
       "Baseline_preDrug_FEV1_FVC    1326\n",
       "status_factorized            1358\n",
       "dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values in column 'Baseline_preDrug_FEV1_FVC'. We can impute the values or simply just drop them. Here we drop the rows which contain missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds[['gender', 'Age_Enroll', 'ageasthonset', 'Baseline_preDrug_FEV1_FVC', 'status_factorized']].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a database only containing the features we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = ds[['gender', 'Age_Enroll', 'ageasthonset', 'Baseline_preDrug_FEV1_FVC']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our target is the asthma status. The columns 'asthma_status' should be the target, however I have already provided the factorized asthma status column in which, each asthma status is mapped to an integer value (1: mild, 2: moderate, 3: severe)\n",
    "\n",
    "I created a class to convert the class numbers back to their names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_class(output):\n",
    "    if output == 1:\n",
    "        print(\"Mild asthma\")\n",
    "    elif output == 2:\n",
    "        print(\"Moderate asthma\")\n",
    "    else:\n",
    "        print(\"Severe asthma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = ds['status_factorized']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gender                       1326\n",
      "Age_Enroll                   1326\n",
      "ageasthonset                 1326\n",
      "Baseline_preDrug_FEV1_FVC    1326\n",
      "dtype: int64\n",
      "status_factorized           1326\n"
     ]
    }
   ],
   "source": [
    "print(X.count())\n",
    "print(str(y.name) + '           ' + str(y.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data should be splitted into train and test sets to provide a possibility of evaluating the model on unknown data (test) for which we assume that we don't have the target value. Then we can evaluate the model by comparing the predictions and actual labels (targets).\n",
    "\n",
    "I set the proportion of test datset to be 0.33 percent of the initial dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Support Vector Machines**\n",
    "Support vector machine are machine learning tools used for classification. They could be used for classes with nonlinear relationships with the target. They could also work as linear classifiers as well. Before using the SVCs, data shoud be scaled so all the features are in the same range. This is crucial for classifiers that work based on eucleadian distance.\n",
    "\n",
    "Here I used one linear and one kernel-based SVC. (The kernel I used is the radial basis function kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "svm_clf = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                   (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\"))])\n",
    "kernel_svm_clf = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                                (\"kernel_clf\", SVC(kernel=\"rbf\", degree=3, coef0=1, C=5))]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the linear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predicting the results for the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluation metrics**\n",
    "We have different evaluation metrics including accuracy, precision, recall, f1_score!\n",
    "\n",
    "refer to the literature for the definitions but as a summary, the higher the better!\n",
    "\n",
    "Confusion matrix shows the class divisions and how many of the instances are missclassified. The best matrix is the one with only values on its diagons, meaning that the classification was perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.6004566210045662\n",
      "Precision is 0.6004566210045662\n",
      "Recall is 0.6004566210045662\n",
      "f1 score is 0.6004566210045662\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score \n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score \n",
    "\n",
    "print('Accuracy is: {0}'.format(svm_clf.score(X_test, y_test)))\n",
    "print('Precision is {0}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Recall is {0}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('f1 score is {0}'.format(f1_score(y_test, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[134,   0,  37],\n",
       "       [ 22,   0,  61],\n",
       "       [ 55,   0, 129]])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting a new target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mild asthma\n"
     ]
    }
   ],
   "source": [
    "res = svm_clf.predict([[1, 45, 7, 0.76]]).astype(np.int32)\n",
    "show_class(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scores and the confusion matrix are not showing satisfactory results. This might be dure to class imbalance, nolinearity or simply mixture of classes.\n",
    "\n",
    "Let's check the class balances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    562\n",
       "1    491\n",
       "2    273\n",
       "Name: status_factorized, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the target values are biased towards mild and severe asthma. We may want to use class weights here to deal with class imbalance. The ratio between classes are mild: 0.37, moderate: 0.21 and severe: 0.42 however we want to put more weight on the lower population classes to avoid bias. Hence I set the class weights argument to 'balanced' to balance the class imbalance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_clf = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                   (\"linear_svc\", LinearSVC(C=1, loss=\"hinge\", class_weight='balanced'))])\n",
    "kernel_svm_clf = Pipeline([(\"scaler\", StandardScaler()), \n",
    "                                (\"kernel_clf\", SVC(kernel=\"rbf\", degree=3, coef0=1, C=5, class_weight='balanced'))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('linear_svc', LinearSVC(C=1, class_weight='balanced', dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='hinge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=None, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.6095890410958904\n",
      "Precision is 0.6095890410958904\n",
      "Recall is 0.6095890410958904\n",
      "f1 score is 0.6095890410958904\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_clf.predict(X_test)\n",
    "print('Accuracy is: {0}'.format(svm_clf.score(X_test, y_test)))\n",
    "print('Precision is {0}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Recall is {0}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('f1 score is {0}'.format(f1_score(y_test, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[139,   0,  32],\n",
       "       [ 24,   0,  59],\n",
       "       [ 56,   0, 128]])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did not work much. Lets try a nonlinear model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('kernel_clf', SVC(C=5, cache_size=200, class_weight='balanced', coef0=1,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_svm_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.6095890410958904\n",
      "Precision is 0.6095890410958904\n",
      "Recall is 0.6095890410958904\n",
      "f1 score is 0.6095890410958904\n"
     ]
    }
   ],
   "source": [
    "y_pred = svm_clf.predict(X_test)\n",
    "print('Accuracy is: {0}'.format(svm_clf.score(X_test, y_test)))\n",
    "print('Precision is {0}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Recall is {0}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('f1 score is {0}'.format(f1_score(y_test, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[139,   0,  32],\n",
       "       [ 24,   0,  59],\n",
       "       [ 56,   0, 128]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K nearest neighbors**\n",
    "\n",
    "Now we use different classifier. K nearest neighbors which works based on the k nearest points to the target to evaluate the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier as knc\n",
    "knn_clf = Pipeline([(\"scaler\", StandardScaler()),\n",
    "                    (\"knc\", knc(weights='distance', n_jobs=-1, n_neighbors=10))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('knc', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=10, p=2,\n",
       "           weights='distance'))])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.6095890410958904\n",
      "Precision is 0.634703196347032\n",
      "Recall is 0.634703196347032\n",
      "f1 score is 0.634703196347032\n"
     ]
    }
   ],
   "source": [
    "y_pred = knn_clf.predict(X_test)\n",
    "print('Accuracy is: {0}'.format(svm_clf.score(X_test, y_test)))\n",
    "print('Precision is {0}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Recall is {0}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('f1 score is {0}'.format(f1_score(y_test, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[120,  14,  37],\n",
       "       [ 21,  24,  38],\n",
       "       [ 33,  17, 134]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moderate asthma\n"
     ]
    }
   ],
   "source": [
    "res = knn_clf.predict([[1, 45, 7, 0.76]]).astype(np.int32)\n",
    "show_class(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It did better for moderate classes but got worse for the other two. How could we know what number of neighbors we should select. Choosing hyperparameters could be a matter of trial and error. However, there is a gridsearch feature we can use to automate it. Let's do it for the number of neighbors in the Knn classifier.\n",
    "\n",
    "cv argument below is for the cross-validation (It estimates the performance of the model by splitting the dataset into K (here 5) folds and takes one of them as unknown target at a time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('scaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('knc', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=-1, n_neighbors=10, p=2,\n",
       "           weights='distance'))]),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid=[{'knc__n_neighbors': [3, 5, 7, 9, 11, 13]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV as gs\n",
    "\n",
    "param_grid = [ {'knc__n_neighbors': [3, 5, 7, 9, 11, 13]} ]\n",
    "grid_search = gs(knn_clf, param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'knc__n_neighbors': 13}"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.6095890410958904\n",
      "Precision is 0.634703196347032\n",
      "Recall is 0.634703196347032\n",
      "f1 score is 0.634703196347032\n"
     ]
    }
   ],
   "source": [
    "y_pred = grid_search.predict(X_test)\n",
    "print('Accuracy is: {0}'.format(svm_clf.score(X_test, y_test)))\n",
    "print('Precision is {0}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Recall is {0}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('f1 score is {0}'.format(f1_score(y_test, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[119,  14,  38],\n",
       "       [ 21,  20,  42],\n",
       "       [ 32,  13, 139]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moderate asthma\n"
     ]
    }
   ],
   "source": [
    "res = grid_search.predict([[1, 45, 7, 0.76]]).astype(np.int32)\n",
    "show_class(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It improved the results slightly. \n",
    "\n",
    "Let's try a decision tree classifier.\n",
    "\n",
    "**Decision trees**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier \n",
    "\n",
    "dt_clf = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.6095890410958904\n",
      "Precision is 0.5365296803652968\n",
      "Recall is 0.5365296803652968\n",
      "f1 score is 0.5365296803652968\n"
     ]
    }
   ],
   "source": [
    "y_pred = dt_clf.predict(X_test)\n",
    "print('Accuracy is: {0}'.format(svm_clf.score(X_test, y_test)))\n",
    "print('Precision is {0}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Recall is {0}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('f1 score is {0}'.format(f1_score(y_test, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,  19,  46],\n",
       "       [ 26,  25,  32],\n",
       "       [ 28,  52, 104]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It doesn't seem that it is improving the results. What if we combine many of these trees and decide beased on the class that gets the highest votes from the trees. This could be done using random forests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random forests**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier \n",
    "rfc_clf = RandomForestClassifier( n_estimators = 500, n_jobs =-1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is: 0.6095890410958904\n",
      "Precision is 0.5365296803652968\n",
      "Recall is 0.5365296803652968\n",
      "f1 score is 0.5365296803652968\n"
     ]
    }
   ],
   "source": [
    "y_pred = dt_clf.predict(X_test)\n",
    "print('Accuracy is: {0}'.format(svm_clf.score(X_test, y_test)))\n",
    "print('Precision is {0}'.format(precision_score(y_test, y_pred, average='micro')))\n",
    "print('Recall is {0}'.format(recall_score(y_test, y_pred, average='micro')))\n",
    "print('f1 score is {0}'.format(f1_score(y_test, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=500, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[106,  19,  46],\n",
       "       [ 26,  25,  32],\n",
       "       [ 28,  52, 104]])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
